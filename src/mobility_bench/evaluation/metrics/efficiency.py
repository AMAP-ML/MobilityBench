"""Efficiency evaluation metric."""


from mobility_bench.evaluation.base import BaseMetric, MetricResult


class EfficiencyMetric(BaseMetric):
    """Efficiency evaluation metric.

    Evaluation dimensions:
    - IT: Input Tokens - total tokens consumed as input context
    - OT: Output Tokens - total tokens generated by the model
    """

    name = "efficiency"
    description = "Efficiency evaluation"

    def __init__(self, config: dict | None = None):
        super().__init__(config)

    def compute(
        self,
        case_id: str,
        prediction: dict,
        ground_truth: dict,
    ) -> MetricResult:
        """Compute single evaluation result."""
        token_usage = prediction.get("token_usage", {})

        # Extract input/output tokens for each stage and sum them
        input_tokens = 0
        output_tokens = 0

        for stage in ["Planner", "Worker", "Reporter"]:
            prompt, completion = self._extract_prompt_completion(token_usage, stage)
            input_tokens += prompt
            output_tokens += completion

        return MetricResult(
            case_id=case_id,
            metric_name=self.name,
            score=0.0,
            details={
                "source_file": ground_truth.get("source_file", ""),
                "intent_family": ground_truth.get("intent_family", ""),
                "IT": input_tokens,
                "OT": output_tokens,
            },
        )

    def _extract_prompt_completion(self, data: dict, stage: str) -> tuple[int, int]:
        """Extract prompt and completion token counts for specific stage."""
        prompt = 0
        completion = 0

        # Try prompt token keys
        prompt_keys = [
            f"{stage}_prompt_tokens",
            f"{stage} Prompt Tokens",
            f"{stage}_input_tokens",
        ]
        for key in prompt_keys:
            if key in data:
                try:
                    prompt = int(data[key])
                    break
                except (ValueError, TypeError):
                    pass

        # Try completion token keys
        completion_keys = [
            f"{stage}_completion_tokens",
            f"{stage} Completion Tokens",
            f"{stage}_output_tokens",
        ]
        for key in completion_keys:
            if key in data:
                try:
                    completion = int(data[key])
                    break
                except (ValueError, TypeError):
                    pass

        # If no prompt/completion found, try total and split
        if prompt == 0 and completion == 0:
            total_keys = [
                f"{stage}_total_tokens",
                f"{stage}_Total_Tokens",
                f"{stage} Total Tokens",
            ]
            for key in total_keys:
                if key in data:
                    try:
                        total = int(data[key])
                        # Approximate split: assume 70% input, 30% output
                        prompt = int(total * 0.7)
                        completion = total - prompt
                        break
                    except (ValueError, TypeError):
                        pass

        return prompt, completion
